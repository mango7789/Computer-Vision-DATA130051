{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d05bad",
   "metadata": {
    "papermill": {
     "duration": 0.005461,
     "end_time": "2024-05-21T09:36:27.687003",
     "exception": false,
     "start_time": "2024-05-21T09:36:27.681542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Load and Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247921a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:27.698571Z",
     "iopub.status.busy": "2024-05-21T09:36:27.698289Z",
     "iopub.status.idle": "2024-05-21T09:36:34.304271Z",
     "shell.execute_reply": "2024-05-21T09:36:34.303149Z"
    },
    "papermill": {
     "duration": 6.615215,
     "end_time": "2024-05-21T09:36:34.307467",
     "exception": false,
     "start_time": "2024-05-21T09:36:27.692252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CUB_ImageFolder(Dataset):\n",
    "    def __init__(self, path: str, transform: transforms.Compose, train: bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an ImageFolder like the one provided in `torchvision.datasets.ImageFolder`.\n",
    "        \n",
    "        Args:\n",
    "        - path: The path of the root directory of the dataset.\n",
    "        - transform: The transform applied to the dataset.\n",
    "        - train: Boolean, return the train dataset if True else test dataset, default is True. \n",
    "        \"\"\"\n",
    "        super(CUB_ImageFolder, self).__init__()\n",
    "        self.root = path\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.train_idx = []\n",
    "        self.test_idx = []\n",
    "        \n",
    "        self._load_dataset()\n",
    "        self._get_train_test()\n",
    "        \n",
    "        self.idx = self.train_idx if self.train else self.test_idx\n",
    "        \n",
    "    def _load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load the image path and corresponding labels from the 'images.txt'\n",
    "        and 'image_class_labels.txt'. \n",
    "        \"\"\"\n",
    "        # load image paths\n",
    "        with open(os.path.join(self.root, 'images.txt')) as f:\n",
    "            for line in f:\n",
    "                self.images.append(line.strip().split()[1])\n",
    "        # load image labels\n",
    "        with open(os.path.join(self.root, 'image_class_labels.txt')) as f:\n",
    "            for line in f:\n",
    "                self.labels.append(line.strip().split()[1])\n",
    "        \n",
    "    def _get_train_test(self):\n",
    "        \"\"\"\n",
    "        Get the indices of the training and testing dataset from the 'train_test_split.txt'.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.root, 'train_test_split.txt')) as f:\n",
    "            for line in f: \n",
    "                idx, is_train = map(int, line.strip().split())\n",
    "                self.train_idx.append(idx) if is_train == 1 else self.test_idx.append(idx)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.idx[index] - 1\n",
    "        image_path, image_label = self.images[image_id], self.labels[image_id]\n",
    "        # get raw images and apply transformation\n",
    "        image_matrix = Image.open(os.path.join(self.root, 'images', image_path)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image_matrix = self.transform(image_matrix)\n",
    "        # convert the returned label into a tensor, here we need \"minius one\" to align with the \n",
    "        # custom that Python's index starts from 0\n",
    "        image_label = torch.tensor(int(image_label) - 1)\n",
    "        return image_matrix, image_label\n",
    "\n",
    "def preprocess_data(batch_size: int=64) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Preprocess the CUB-200-2011 dataset and return the train and test 'DataLoader'.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size: The number of samples in one batch, default is 64.\n",
    "    \"\"\"\n",
    "    # resize and normalize the images. Apply data augumentation to the training dataset.\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(degrees=(-15, 15)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    data_dir = '/kaggle/input/cub2002011/CUB_200_2011'\n",
    "\n",
    "    # load the dataset and extract the train/test Dataloader\n",
    "    train_dataset = CUB_ImageFolder(data_dir, transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = CUB_ImageFolder(data_dir, transform=test_transform, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91783173",
   "metadata": {
    "papermill": {
     "duration": 0.007298,
     "end_time": "2024-05-21T09:36:34.322243",
     "exception": false,
     "start_time": "2024-05-21T09:36:34.314945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Define `ResNet18` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56120647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:34.337455Z",
     "iopub.status.busy": "2024-05-21T09:36:34.336559Z",
     "iopub.status.idle": "2024-05-21T09:36:34.346502Z",
     "shell.execute_reply": "2024-05-21T09:36:34.345534Z"
    },
    "papermill": {
     "duration": 0.019607,
     "end_time": "2024-05-21T09:36:34.348553",
     "exception": false,
     "start_time": "2024-05-21T09:36:34.328946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torchvision.models as models\n",
    "\n",
    "class CUB_ResNet_18(nn.Module):\n",
    "    def __init__(self, num_classes: int=200, pretrain: bool=True):\n",
    "        \"\"\"\n",
    "        Create a neural network with the same architecture as ResNet-18. The output layer is \n",
    "        resized to (`in_features`, `num_classes`) to fit into the specific dataset.\n",
    "        \n",
    "        Args:\n",
    "        - num_classes: Number of classes(labels), default is 200.\n",
    "        - pretrain: Boolean, whether the paramters of ResNet-18 is pretrained or not. Default\n",
    "        is True.\n",
    "        \"\"\"\n",
    "        super(CUB_ResNet_18, self).__init__()\n",
    "        # initialize the parameters\n",
    "        if pretrain:\n",
    "            self.resnet18 = models.resnet18(weights=\"ResNet18_Weights.IMAGENET1K_V1\")\n",
    "        else:\n",
    "            self.resnet18 = models.resnet18(weights=None)\n",
    "            \n",
    "        # change the output layer\n",
    "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
    "        \n",
    "        # apply Kaiming initialization to the fully connected layer\n",
    "        init.kaiming_normal_(self.resnet18.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if self.resnet18.fc.bias is not None:\n",
    "            nn.init.constant_(self.resnet18.fc.bias, 0)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \"\"\"\n",
    "        return self.resnet18(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c8279",
   "metadata": {
    "papermill": {
     "duration": 0.005201,
     "end_time": "2024-05-21T09:36:34.359143",
     "exception": false,
     "start_time": "2024-05-21T09:36:34.353942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Define Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157bc66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:34.372490Z",
     "iopub.status.busy": "2024-05-21T09:36:34.372053Z",
     "iopub.status.idle": "2024-05-21T09:36:45.300799Z",
     "shell.execute_reply": "2024-05-21T09:36:45.300005Z"
    },
    "papermill": {
     "duration": 10.938663,
     "end_time": "2024-05-21T09:36:45.302989",
     "exception": false,
     "start_time": "2024-05-21T09:36:34.364326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def seed_everything(seed: int=None):\n",
    "    \"\"\"\n",
    "    Set the random seed for the whole neural network.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_data_model_criterion(pretrain: bool=True) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the DataLoader, model and loss criterion.\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    train_loader, test_loader = preprocess_data()\n",
    "\n",
    "    # get the pretrained model\n",
    "    model = CUB_ResNet_18(pretrain=pretrain)\n",
    "\n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return train_loader, test_loader, model, criterion\n",
    "\n",
    "def calculate_topk_correct(output: torch.Tensor, target: torch.Tensor, topk=(1, 5)) -> list[int]:\n",
    "    \"\"\"\n",
    "    Computes the top-k correct samples for the specified values of k.\n",
    "\n",
    "    Args:\n",
    "    - output (torch.Tensor): The model predictions with shape (batch_size, num_classes).\n",
    "    - target (torch.Tensor): The true labels with shape (batch_size, ).\n",
    "    - topk (tuple): A tuple of integers specifying the top-k values to compute.\n",
    "\n",
    "    Returns:\n",
    "    - List of top-k correct samples for each value in topk.\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    # get the indices of the top k predictions\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.item())\n",
    "    return res\n",
    "\n",
    "def train_resnet_with_cub(\n",
    "    num_epochs: list[int], \n",
    "    fine_tuning_lr: float=0.0001, \n",
    "    output_lr: float=0.001, \n",
    "    pretrain: bool=True, \n",
    "    save: bool=False,\n",
    "    **kwargs: dict\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Train the modified ResNet-18 model using the CUB-200-2011 dataset and return the best accuracy.\n",
    "    Some hyper-parameters can be modified here.\n",
    "    \n",
    "    Args:\n",
    "    - num_epochs: A list of number of training epochs.\n",
    "    - fine_tuning_lr: Learning rate of the parameters outside the output layer, default is 0.0001.\n",
    "    - output_lr: Learning rate of the parameters inside the output layer, default is 0.001.\n",
    "    - pretrain: Boolean, whether the ResNet-18 model is pretrained or not. Default is True.\n",
    "    - save: Boolean, whether the parameters of the best model will be save. Default is False.\n",
    "    \n",
    "    Return:\n",
    "    - best_acc: The best validation accuracy list during the training process.\n",
    "    \"\"\"\n",
    "    # set the random seed\n",
    "    seed_everything(kwargs.pop('seed', 42))\n",
    "    \n",
    "    # get the dataset, model and loss criterion\n",
    "    train_loader, test_loader, model, criterion = get_data_model_criterion(pretrain)\n",
    "    \n",
    "    # move the model to CUDA (GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # get the parameters of the model expect the last layer\n",
    "    former_params = [p for name, p in model.resnet18.named_parameters() if 'fc' not in name]\n",
    "    \n",
    "    # pop the hyper-parameters from the kwargs dict\n",
    "    momentum = kwargs.pop('momentum', 0.9)\n",
    "    weight_decay = kwargs.pop('weight_decay', 1e-4)\n",
    "        \n",
    "    # define optimizer\n",
    "    optimizer = optim.SGD([\n",
    "                {'params': former_params, 'lr': fine_tuning_lr, 'weight_decay': weight_decay},\n",
    "                {'params': model.resnet18.fc.parameters(), 'lr': output_lr, 'weight_decay': weight_decay}\n",
    "            ], momentum=momentum\n",
    "        )\n",
    "    \n",
    "    # scheduler step size and gamma\n",
    "    step_size = kwargs.pop('step', 30)\n",
    "    gamma = kwargs.pop('gamma', 0.1)\n",
    "\n",
    "    # custom step scheduler\n",
    "    def custom_step_scheduler(optimizer: optim, epoch: int, step_size: int, gamma: float):\n",
    "        \"\"\"\n",
    "        Decay the learning rate of the second parameter group by gamma every step_size epochs.\n",
    "        \"\"\"\n",
    "        if epoch % step_size == 0 and epoch > 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= gamma\n",
    "    \n",
    "    # init the tensorboard\n",
    "    tensorboard_name = \"/kaggle/working/Fine_Tuning_With_Pretrain\"\n",
    "    if len(num_epochs) != 1:\n",
    "        tensorboard_name = \"/kaggle/working/Full_Train\"\n",
    "    writer = SummaryWriter(tensorboard_name, comment=\"-{}-{}\".format(fine_tuning_lr, output_lr))\n",
    "        \n",
    "    # best accuracy\n",
    "    best_acc = 0.0\n",
    "    store_best_acc, count = [0 for _ in range(len(num_epochs))], 0\n",
    "    max_num_epoch = max(num_epochs)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training with configuration ({:>7.5f}, {:>7.5f})\".format(fine_tuning_lr, output_lr))\n",
    "    \n",
    "    # iterate\n",
    "    for epoch in range(max_num_epoch):\n",
    "        # train\n",
    "        model.train()\n",
    "        samples = 0\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            samples += inputs.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # learning rate decay\n",
    "        custom_step_scheduler(optimizer, epoch, step_size, gamma)\n",
    "        \n",
    "        train_loss = running_loss / samples\n",
    "        print(\"[Epoch {:>2} / {:>2}], Training loss is {:>8.6f}\".format(epoch + 1, max_num_epoch, train_loss))\n",
    "\n",
    "        # test\n",
    "        model.eval()\n",
    "        correct_top1 = 0\n",
    "        correct_top5 = 0\n",
    "        samples = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                top1, top5 = calculate_topk_correct(outputs, labels, topk=(1, 5))\n",
    "                correct_top1 += top1\n",
    "                correct_top5 += top5\n",
    "                samples += labels.size(0)\n",
    "                \n",
    "                running_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "        # add loss and accuracy to tensorboard\n",
    "        test_loss = running_loss / samples\n",
    "        writer.add_scalars('Loss', {'Train': train_loss, 'Valid': test_loss}, epoch + 1)\n",
    "        accuracy_top1 = correct_top1 / samples\n",
    "        accuracy_top5 = correct_top5 / samples\n",
    "        writer.add_scalars(\n",
    "            'Valid Accuracy', \n",
    "            {\n",
    "                'Top1': accuracy_top1,\n",
    "                'Top5': accuracy_top5,\n",
    "            },\n",
    "            epoch + 1\n",
    "            \n",
    "        )\n",
    "        \n",
    "        print(\"[Epoch {:>2} / {:>2}], Validation loss is {:>8.6f}, Top-5 accuracy is {:>8.6f}, Top-1 accuracy is {:>8.6f}\".format(\n",
    "            epoch + 1, max_num_epoch, test_loss, accuracy_top5, accuracy_top1\n",
    "        ))\n",
    "        \n",
    "        # update the best accuracy and save the model if it improves\n",
    "        if accuracy_top1 > best_acc:\n",
    "            best_acc = accuracy_top1\n",
    "            if save:\n",
    "                torch.save(model.state_dict(), 'resnet18_cub.pth')\n",
    "            \n",
    "        if epoch + 1 == num_epochs[count]:\n",
    "            store_best_acc[count] = best_acc\n",
    "            count += 1\n",
    "\n",
    "    # close the tensorboard\n",
    "    writer.close()\n",
    "    \n",
    "    return store_best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262dd511",
   "metadata": {
    "papermill": {
     "duration": 0.004886,
     "end_time": "2024-05-21T09:36:45.313349",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.308463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052dbaf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:45.324717Z",
     "iopub.status.busy": "2024-05-21T09:36:45.324209Z",
     "iopub.status.idle": "2024-05-21T09:36:45.328619Z",
     "shell.execute_reply": "2024-05-21T09:36:45.327723Z"
    },
    "papermill": {
     "duration": 0.012209,
     "end_time": "2024-05-21T09:36:45.330537",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.318328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "\n",
    "KAGGLE_INPUT_DIR = \"cub2002011/CUB_200_2011\"\n",
    "KAGGLE_PATH = os.path.join(\"/kaggle\", \"input\", KAGGLE_INPUT_DIR)\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec719a6",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2024-05-21T09:36:45.395195",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.390215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Grid-Search Optimal Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a8369",
   "metadata": {},
   "source": [
    "#### 5.1 Set the range of hyper-parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd38fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:45.406404Z",
     "iopub.status.busy": "2024-05-21T09:36:45.406029Z",
     "iopub.status.idle": "2024-05-21T09:36:45.409638Z",
     "shell.execute_reply": "2024-05-21T09:36:45.408859Z"
    },
    "papermill": {
     "duration": 0.011256,
     "end_time": "2024-05-21T09:36:45.411480",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.400224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set hyper-parameters here\n",
    "num_epochs = [15]\n",
    "fine_tuning_lrs = [5e-5, 1e-4, 5e-4]\n",
    "output_lrs = [1e-3, 2e-3, 5e-3, 8e-3, 1e-2]\n",
    "\n",
    "configurations = list(product(fine_tuning_lrs, output_lrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5a000",
   "metadata": {
    "papermill": {
     "duration": 0.005585,
     "end_time": "2024-05-21T09:36:45.422168",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.416583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6384001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:45.433225Z",
     "iopub.status.busy": "2024-05-21T09:36:45.432720Z",
     "iopub.status.idle": "2024-05-21T09:36:45.436493Z",
     "shell.execute_reply": "2024-05-21T09:36:45.435701Z"
    },
    "papermill": {
     "duration": 0.011134,
     "end_time": "2024-05-21T09:36:45.438231",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.427097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_accs = []\n",
    "\n",
    "# train with the pretrained model\n",
    "for config in configurations:\n",
    "    curr_best_acc = train_resnet_with_cub(num_epochs, fine_tuning_lr=config[0], output_lr=config[1])\n",
    "    best_accs.extend(curr_best_acc)\n",
    "\n",
    "best_acc = max(best_accs)    \n",
    "best_fine_tune_lr, best_output_lr = None, None\n",
    "\n",
    "# write the results into a txt file\n",
    "with open('best_accuracy_lr.txt', 'w') as f:\n",
    "    f.write(\"Configuration        Accuracy\\n\")\n",
    "    f.write(\"=\" * 30 + \"\\n\")\n",
    "    for config, accuracy in zip(configurations, best_accs):\n",
    "        # find the best config\n",
    "        if accuracy == best_acc and best_fine_tune_lr is None:\n",
    "            best_fine_tune_lr, best_output_lr = config\n",
    "        # write the training record into file \n",
    "        f.write(\"({:>7.5f}, {:>7.5f})   {:>8.6f}\\n\".format(config[0], config[1], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51d189",
   "metadata": {},
   "source": [
    "#### 5.3 Inspect the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !more best_accuracy_lr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6a831",
   "metadata": {
    "papermill": {
     "duration": 0.004855,
     "end_time": "2024-05-21T09:36:45.448259",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.443404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Find the Optimal Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f640a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:36:45.459717Z",
     "iopub.status.busy": "2024-05-21T09:36:45.459461Z",
     "iopub.status.idle": "2024-05-21T12:21:21.577410Z",
     "shell.execute_reply": "2024-05-21T12:21:21.576463Z"
    },
    "papermill": {
     "duration": 9876.149724,
     "end_time": "2024-05-21T12:21:21.603232",
     "exception": false,
     "start_time": "2024-05-21T09:36:45.453508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = [15, 30, 45]\n",
    "\n",
    "# init the configurations\n",
    "best_accs = []\n",
    "\n",
    "# train with the pretrained model\n",
    "curr_best_acc = train_resnet_with_cub(num_epochs, best_fine_tune_lr, best_output_lr, save=True)\n",
    "best_accs.extend(curr_best_acc)\n",
    "    \n",
    "# write the results into a txt file\n",
    "with open('best_accuracy_ep.txt', 'w') as f:\n",
    "    f.write(\"Epoch  Accuracy\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for num_epoch, accuracy in zip(num_epochs, best_accs):\n",
    "        f.write(\"{:>2}     {:>8.6f}\\n\".format(num_epoch, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84d599",
   "metadata": {},
   "source": [
    "### 7. Train with Random Initialized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_best_acc = train_resnet_with_cub(num_epochs, best_fine_tune_lr, best_output_lr, pretrain=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2534241,
     "sourceId": 5140550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9900.128476,
   "end_time": "2024-05-21T12:21:25.036629",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T09:36:24.908153",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
