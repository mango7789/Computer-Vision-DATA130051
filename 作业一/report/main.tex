%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------
%     BIBLIOGRAPHY WITH BIBLATEX IN EXTERNAL EDITORS
% --------------------------------------------------------
% If the bibliography does not show up, try running the 
% 'tau.cls' and 'tau.bib' file with biber from the 
% MikTeX console or your preferred LaTeX distribution to 
% generate the auxiliar files and (re)run the main.tex.
% --------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------
%					  FOR SPANISH BABEL
% --------------------------------------------------------
% \usepackage[spanish,es-nodecimaldot,es-noindentfirst]{babel}
% --------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,a4paper,twoside]{tau}
\usepackage[english]{babel}
\usepackage{tauenvs}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\d}{\text{d}}
%----------------------------------------------------------
% TITLE
%----------------------------------------------------------

\title{Image Classification Based On Three-Layer Neural Network}

%----------------------------------------------------------
% AUTHORS, AFFILIATIONS AND PROFESSOR
%----------------------------------------------------------

\author[]{Xiang Zheng, 21307110169}

%----------------------------------------------------------

\affil[]{School of Data Science, Fudan University}

%----------------------------------------------------------
% FOOTER INFORMATION
%----------------------------------------------------------

\institution{Fudan University}
\ftitle{}
\date{\today}
\etal{Author last name et al.}
\course{Computer Vision}

%----------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------

\begin{abstract}    
    Welcome to tau ($\tau$) \LaTeX\ class for making academic articles and lab reports. In this example template, we will guide you through the process of using and customizing this class to your needs. For more information of this class check out the appendix section. There, you will find snippets codes that define key aspects of the template, allowing you to explore and modify them.
\end{abstract}

%----------------------------------------------------------

\keywords{\LaTeX\ class, lab report, academic article, tau class}

%----------------------------------------------------------

\begin{document}
		
\maketitle
\thispagestyle{firststyle}
\tauabstract
%----------------------------------------------------------

\section{Introduction}

\taustart{I}n this project, we aim to develop and train a fully connected neural network to classify images from the Fashion-MNIST dataset. The Fashion-MNIST dataset, curated by Zalando, comprises a collection of grayscale images representing various fashion items, along with corresponding labels indicating their respective categories. With 60,000 training examples and 10,000 test examples, this dataset serves as a standard benchmark for evaluating machine learning algorithms in the context of fashion image classification.

The neural network architecture we will employ consists of three linear layers, which will be trained to learn meaningful representations from the input images and make accurate predictions about their corresponding classes. Throughout this project, we will explore different components of the neural network, including activation functions, loss functions, and training strategies, to optimize its performance on the Fashion-MNIST dataset.

The remainder of this report is structured as follows: Section 2 provides the downloading the pre-process method of the Fashion-MNIST dataset. Section 3 delves into the specifics of the neural network architecture, detailing the design choices for its components. Section 4 outlines the training process, including the selection of hyperparameters and evaluation metrics. In Section 5, we evaluate the trained model on the test dataset and present visualizations to assess its effectiveness. Finally, Section 6 offers concluding remarks and discusses potential avenues for future research and improvement.

\section{Dataset}

Having already covered the dataset's characteristics earlier, we'll now delve straight into the downloading and preprocessing steps. Utilizing the Python library \texttt{urllib}, we'll fetch the data from its \href{https://github.com/zalandoresearch/fashion-mnist}{official website} and organize it into a dictionary containing images and labels. Each numpy array representing a grayscale image will be reshaped into the format $(1, 28, 28)$ to maintain consistency with their original shapes.

\section{Model}

A standard linear layer neural network comprises three main components: the input layer, hidden layers, and output layer. The transition between the input (hidden) layer and hidden layers can be described as a \texttt{LinearActivation} layer, while the transition between the hidden layer and output layer involves a \texttt{Linear} layer along with a \texttt{loss} function. Other parameters like \texttt{reg} and \texttt{weight\_scale} also influence the performance of the model.

Before delving into the discussion of our model, it's essential to establish the key notations utilized throughout this report. Let $N$ represent the batch size, indicating the number of samples processed in each training or inference iteration. $D$ denotes the flattened dimension of the image, symbolizing the size of the image vector after flattening. $H_1, H_2, \cdots, H_n$ refer to the dimensions of the hidden layers within the neural network architecture. Finally, $C$ signifies the output dimension, representing the number of classes in the classification task.

\subsection{Linear Layer}

Consider the first linear layer as an illustration. The forward pass can be concisely expressed as:

\begin{equation}
    \vec{Y} = \vec{X}\vec{W} + \vec{b} \tag{3.1.1}
\end{equation}

Here, $\vec{Y} \in \mathbb{R}^{N \times H_1}$, $\vec{X} \in \mathbb{R}^{N \times D}$, $\vec{W} \in \mathbb{R}^{D \times H_1}$, and $\vec{b} \in \mathbb{R}^{H_1}$. It's worth noting that \texttt{numpy} broadcasts $\vec{b}$ to fit into the computation. Hence, I represent $\vec{b}$ as a vector to maintain alignment with my code implementation. Also, please note that we need to 'cache' the current values of $\vec{X}$ and $\vec{W}$ for the backward pass.

Regarding the backward pass, the input of the backward function is \texttt{dout} and \texttt{cache}, where \texttt{dout} represents the gradient at $\vec{Y}$ with shape $(N, H_1)$. Taking derivatives with respect to $\vec{X}$ and $\vec{W}$, we obtain:

\begin{equation}
    \d\vec{X} = \d\vec{Y} \cdot \vec{W}^{\top} \tag{3.1.2}
\end{equation}
\begin{equation}
    \d\vec{W} = \vec{X}^{\top} \cdot \d\vec{Y} \tag{3.1.3}
\end{equation}
For bias $\vec{b}$, the derivative involves summing \texttt{dout} over the first dimension. 

You can observe that the shapes of these matrices match perfectly. The same technique can be applied to other following linear layers. Feel free to refer to my code in \texttt{linear\_layer.py} to see the details of implementation.


\subsection{Activation Functions}

When it comes to activation functions, they all perform an element-wise operation on the given matrix. There are several commonly used types of activation functions, including \texttt{ReLU}, \texttt{Tanh}, and \texttt{Sigmoid}. Let's introduce each of them individually.

\subsubsection{ReLU}

For the forward pass, ReLU is expressed as:
\begin{equation}
\vec{Y'} = \max(\vec{Y}, \vec{0}) \tag{3.2.1}
\end{equation}
For the backward pass, it's:
\begin{equation}
\d\vec{Y} = \max(\d\vec{Y'}, \vec{0}) \tag{3.2.2}
\end{equation}

\subsubsection{Tanh}

The mathematical expression of the hyperbolic tangent function, Tanh, is:
\begin{equation}
\tanh{x} = \frac{e^{2x} - 1}{e^{2x} + 1} \tag{3.2.3}
\end{equation}
Taking the derivative with respect to $x$, we get:
\begin{equation}
\frac{\partial \tanh{x}}{\partial x} = \frac{4}{(e^{2x}+1)^2} = 1 - \tanh^2{x} \tag{3.2.4}
\end{equation}

The forward and backward pass follows directly from the above equations:
\begin{equation}
\vec{Y'} = \tanh{\vec{Y}} \tag{3.2.5}
\end{equation}
\begin{equation}
\d\vec{Y} = \d\vec{Y'}\cdot(1 - \tanh^2\vec{Y}) \tag{3.2.6}
\end{equation}

\subsubsection{Sigmoid}

Let's denote the sigmoid function as:
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}} \tag{3.2.7}
\end{equation}
Taking the derivative with respect to $x$, we obtain:
\begin{equation}
\frac{\partial\sigma(x)}{\partial x} = \frac{e^{-x}}{(1 + e^{-x})^2} = \sigma(x)(1 - \sigma(x)) \tag{3.2.8}
\end{equation}
Then, the forward and backward pass of the sigmoid function are:
\begin{equation}
\vec{Y'} = \sigma(\vec{Y}) \tag{3.2.9}
\end{equation}
\begin{equation}
\d\vec{Y} = \d\vec{Y'}\cdot \sigma(\vec{Y}) (1 - \sigma(\vec{Y})) \tag{3.2.10}
\end{equation}

\subsection{Loss Function}

The loss function plays a crucial role in training a neural network as it quantifies the discrepancy between the predicted output and the actual target. Here, I'll implement the \texttt{cross\_entrophy}. Since this is a multi-class classification task, the CE is slightly different from the binary case.

For traditional binary cross entropy, the loss function is

\begin{equation}
    \text{loss}(\vec{y}, \vec{\hat{y}}) = -\frac{1}{N} \sum_{i=1}^{n}y_i
    \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \tag{3.3.1}
\end{equation}

where $y_i$ can only take the value of 0 or 1. However, this is not appropriate in multi-class classification.

To compute the loss function in the forward pass of the neural network, we'll first apply the softmax function to the raw output scores to obtain the predicted probabilities for each class. The softmax function is defined as:

\begin{equation}
\text{softmax}(\vec{x})_i = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}} \tag{3.3.2}
\end{equation}

where $\vec{x}$ is the vector of raw output scores, and $C$ is the number of classes. This function ensures that the predicted probabilities are normalized and sum up to $1$ for each sample. Besides, the 'shifted x' method is used here to avoid overflow.

Once the softmax probabilities are computed, the cross-entropy loss can be calculated using the true labels and the predicted probabilities. The cross-entropy loss is defined as:

\begin{equation}
\text{CE} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{s=1}^{C} t_{i,s} \log(f(s)_i) \tag{3.3.3}
\end{equation}

where $N$ is the number of samples, $C$ is the number of classes, $t_{i,s}$ is the true label for the $i$-th sample and $s$-th class (which is $1$ if the sample belongs to class $s$ and $0$ otherwise), and $f(s)_i$ is the predicted probability for the $i$-th sample belonging to the $s$-th class. Hence the loss can be further simplified as
\begin{equation}
    \text{CE} = -\frac{1}{N} \sum_{i=1}^{N} t_i \log(f(s)_i) \tag{3.3.4}
\end{equation}

In the backward pass, we'll need to compute the gradient of the loss function with respect to the raw output scores. This involves subtracting the true labels from the softmax probabilities, resulting in the gradient matrix. The gradient can be calculated as:

\begin{equation}
\frac{\partial \text{CE}}{\partial \mathbf{x}} = \frac{1}{N} (\text{softmax}(\mathbf{x}) - \mathbf{t}) \tag{3.3.5}
\end{equation}

\subsection{Other Parameters}

In addition to the model architecture like linear, activation layers, neural networks often involve hyper-parameters such as \texttt{reg} and \texttt{weight\_scale}. These parameters control regularization and weight initialization, respectively.

\texttt{reg} is a regularization parameter that helps prevent overfitting by penalizing large weights in the model. In the next section, I'll discuss how grid search is used to find the best value for \texttt{reg}.

For the initial scale of the weight matrix, a commonly recommended value is 0.01. This value provides a good starting point for weight initialization, ensuring that the network begins training with reasonable weights.


\section{Train}
\section{Load \& Test}
\section{Discussion}



% \section{Appendix}

%     \subsection{Environments preview}

%         The following environments are defined in \textit{tauenvs} package.
		
% 		\subsubsection{Tau environment}

%                 The following code defines the tauenv environment. A custom title can be added to this environment.

% 			\begin{tauenv}[frametitle=Tauenv]
%                     Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed vestibulum justo quis massa aliquet, ut ultrices quam bibendum.
% 			\end{tauenv}
		
% \begin{lstlisting}[language=TeX, caption=Tauenv environment code.]
% \newmdenv[
% 	backgroundcolor=taublue!22, 					
% 	linecolor=taublue,									
% 	linewidth=0.7pt,
% 	frametitle=\vskip0pt\bfseries,
% 	frametitlerule=false,
% 	frametitlefont=\color{taublue}\bfseries\sffamily,
% 	frametitlealignment=\raggedright,
% 	innertopmargin=3pt,
% 	innerbottommargin=6pt,
% 	innerleftmargin=6pt,
% 	innerrightmargin=6pt,
% 	font=\selectfont,
% 	fontcolor=taublue,									
% 	frametitleaboveskip=8pt,
% 	skipabove=10pt
% ]{tauenv} \end{lstlisting}
		
% 		\subsubsection{Note}

%                 This code defines the note environment.

%   			\begin{note}
%                     Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed vestibulum justo quis massa aliquet, ut ultrices quam bibendum.
% 			\end{note}
		
% \begin{lstlisting}[language=TeX, caption=Note environment code.]
% \newmdenv[
% 	backgroundcolor=taublue!22, 						
% 	linecolor=taublue,									
% 	linewidth=0.7pt,
% 	frametitle=\vskip0pt\bfseries\notelanguage,
% 	frametitlerule=false,
% 	frametitlefont=\color{taublue}\bfseries\sffamily,
% 	frametitlealignment=\raggedright,
% 	innertopmargin=3pt,
% 	innerbottommargin=6pt,
% 	innerleftmargin=6pt,
% 	innerrightmargin=6pt,
% 	font=\normalfont,
% 	fontcolor=taublue,									
% 	frametitleaboveskip=3pt,
% 	skipabove=10pt
% ]{note} \end{lstlisting}

% 		\subsubsection{Info}

%                 This code defines the info environment.

%     		\begin{info}
%                     Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed vestibulum justo quis massa aliquet, ut ultrices quam bibendum.
% 			\end{info}
		
% \begin{lstlisting}[language=TeX, caption=Info environment code.]
% \newmdenv[
% 	backgroundcolor=taublue!22, 						
% 	linecolor=taublue,									
% 	linewidth=0.7pt,
% 	frametitle=\vskip0pt\bfseries\infolanguage,
% 	frametitlerule=false,
% 	frametitlefont=\color{taublue}\bfseries\sffamily,
% 	frametitlealignment=\raggedright,
% 	innertopmargin=3pt,
% 	innerbottommargin=6pt,
% 	innerleftmargin=6pt,
% 	innerrightmargin=6pt,
% 	font=\normalfont,
% 	fontcolor=taublue,									
% 	frametitleaboveskip=3pt,
% 	skipabove=10pt
% ]{info} \end{lstlisting}

%     \subsection{Alternative title}

%          You can make the following modification to \textit{tau class} in the \textit{title preferences} section to change the position of the title. This will move the title to the left. 

% \begin{lstlisting}[language=TeX, caption=Alternative title.]
% \renewcommand{\@maketitle}{%
%         \vskip-18pt
%     {\RaggedRight\bfseries\color{taublue}\fontsize{18}{22}\sffamily\selectfont\@title\par}
% 		\vskip8pt
%     {\RaggedRight\normalsize\sffamily\@author\par}
%         \vskip8pt
%     {\RaggedRight\fontsize{7pt}{8pt}\selectfont\@professor\par}
%         \vskip24pt
% }% 
% \end{lstlisting}

%     \subsection{Equation skip value}

%         Play with the value of \verb|\eqskip| until the preferred spacing is set for equations.

% \begin{lstlisting}[language=TeX, caption=Equation skip code.]
% \newlength{\eqskip}\setlength{\eqskip}{6.5pt}
% \expandafter\def\expandafter\normalsize\expandafter{%
%     \normalsize%
%     \setlength\abovedisplayskip{\eqskip}%
%     \setlength\belowdisplayskip{\eqskip}%
%     \setlength\abovedisplayshortskip{\eqskip-\baselineskip}%
%     \setlength\belowdisplayshortskip{\eqskip}%
% }
% \end{lstlisting}
					
% %----------------------------------------------------------

% \addcontentsline{toc}{section}{References}
% \printbibliography

% %----------------------------------------------------------

% \begin{center}
% 	\vskip10pt
% 	Enjoy writing with tau \LaTeX\ class $\blacksmiley$ \\ 
% 	\vskip10pt
% 	\textit{Contact:} \\
% 	\faLink\ \href{https://sites.google.com/view/memo-notess/p%C3%A1gina-principal}{https://sites.google.com/memo-notess} \\
% 	\faEnvelope[regular]\ memo.notess1@gmail.com \\
% 	\faInstagram\ memo.notess\\
% \end{center}

% %----------------------------------------------------------

\end{document}