{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CUB_ImageFolder(Dataset):\n",
    "    def __init__(self, path: str, transform: transforms.Compose, train: bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an ImageFolder like the one provided in `torchvision.datasets.ImageFolder`.\n",
    "        \n",
    "        Args:\n",
    "        - path: The path of the root directory of the dataset.\n",
    "        - transform: The transform applied to the dataset.\n",
    "        - train: Boolean, return the train dataset if True else test dataset, default is True. \n",
    "        \"\"\"\n",
    "        super(CUB_ImageFolder, self).__init__()\n",
    "        self.root = path\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.train_idx = []\n",
    "        self.test_idx = []\n",
    "        \n",
    "        self._load_dataset()\n",
    "        self._get_train_test()\n",
    "        \n",
    "        self.idx = self.train_idx if self.train else self.test_idx\n",
    "        \n",
    "    def _load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load the image path and corresponding labels from the 'images.txt'\n",
    "        and 'image_class_labels.txt'. \n",
    "        \"\"\"\n",
    "        # load image paths\n",
    "        with open(os.path.join(self.root, 'images.txt')) as f:\n",
    "            for line in f:\n",
    "                self.images.append(line.strip().split()[1])\n",
    "        # load image labels\n",
    "        with open(os.path.join(self.root, 'image_class_labels.txt')) as f:\n",
    "            for line in f:\n",
    "                self.labels.append(line.strip().split()[1])\n",
    "        \n",
    "    def _get_train_test(self):\n",
    "        \"\"\"\n",
    "        Get the indices of the training and testing dataset from the 'train_test_split.txt'.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.root, 'train_test_split.txt')) as f:\n",
    "            for line in f: \n",
    "                idx, is_train = map(int, line.strip().split())\n",
    "                self.train_idx.append(idx) if is_train == 1 else self.test_idx.append(idx)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.idx[index] - 1\n",
    "        image_path, image_label = self.images[image_id], self.labels[image_id]\n",
    "        # get raw images and apply transformation\n",
    "        image_matrix = Image.open(os.path.join(self.root, 'images', image_path)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image_matrix = self.transform(image_matrix)\n",
    "        # convert the returned label into a tensor, here we need \"minius one\" to align with the \n",
    "        # custom that Python's index starts from 0\n",
    "        image_label = torch.tensor(int(image_label) - 1)\n",
    "        return image_matrix, image_label\n",
    "\n",
    "\n",
    "def preprocess_data(batch_size: int=128) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Preprocess the CUB-200-2011 dataset and return the train and test 'DataLoader'.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size: The number of samples in one batch, default is 128.\n",
    "    \"\"\"\n",
    "    \n",
    "    # resize and normalize the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    data_dir = 'data/CUB_200_2011'\n",
    "\n",
    "    # load the dataset and extract the train/test Dataloader\n",
    "    train_dataset = CUB_ImageFolder(data_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = CUB_ImageFolder(data_dir, transform=transform, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CUB_ResNet_18(nn.Module):\n",
    "    def __init__(self, num_classes: int=200, pretrain: bool=True):\n",
    "        \"\"\"\n",
    "        Create a neural network with the same architecture as ResNet-18. The output layer is \n",
    "        resized to (`in_features`, `num_classes`) to fit into the specific dataset.\n",
    "        \n",
    "        Args:\n",
    "        - num_classes: Number of classes(labels), default is 200.\n",
    "        - pretrain: Boolean, whether the paramters of ResNet-18 is pretrained or not. Default\n",
    "        is True.\n",
    "        \"\"\"\n",
    "        super(CUB_ResNet_18, self).__init__()\n",
    "        # initialize the parameters\n",
    "        if pretrain:\n",
    "            self.resnet18 = models.resnet18(weights=\"ResNet18_Weights.IMAGENET1K_V1\")\n",
    "        else:\n",
    "            self.resnet18 = models.resnet18(weights=None)\n",
    "        # change the output layer\n",
    "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \"\"\"\n",
    "        return self.resnet18(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.manual_seed(509)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_data_model_criterion(pretrain: bool=True) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the DataLoader, model and loss criterion.\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    train_loader, test_loader = preprocess_data()\n",
    "\n",
    "    # get the pretrained model\n",
    "    model = CUB_ResNet_18(pretrain=pretrain)\n",
    "\n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return train_loader, test_loader, model, criterion\n",
    "\n",
    "\n",
    "def train_resnet_with_cub(num_epochs: list[int], fine_tuning_lr: float=0.0001, output_lr: float=0.001, pretrain: bool=True, **kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Train the modified ResNet-18 model using the CUB-200-2011 dataset and return the best accuracy.\n",
    "    Some hyper-parameters can be modified here.\n",
    "    \n",
    "    Args:\n",
    "    - num_epochs: A list of number of training epochs.\n",
    "    - fine_tuning_lr: Learning rate of the parameters outside the output layer, default is 0.0001.\n",
    "    - output_lr: Learning rate of the parameters inside the output layer, default is 0.001.\n",
    "    - pretrain: Boolean, whether the ResNet-18 model is pretrained or not. Default is True.\n",
    "    \n",
    "    Return:\n",
    "    - best_acc: The best validation accuracy during the training process.\n",
    "    \"\"\"\n",
    "    # get the dataset, model and loss criterion\n",
    "    train_loader, test_loader, model, criterion = get_data_model_criterion(pretrain)\n",
    "    \n",
    "    # move the model to CUDA (GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # get the parameters of the model expect the last layer\n",
    "    former_params = [p for name, p in model.resnet18.named_parameters() if 'fc' not in name]\n",
    "    \n",
    "    # pop the hyper-parameters from the kwargs dict\n",
    "    momentum = kwargs.pop('momentum', 0.9)\n",
    "        \n",
    "    # define optimizer\n",
    "    optimizer = optim.SGD([\n",
    "                {'params': former_params, 'lr': fine_tuning_lr},\n",
    "                {'params': model.resnet18.fc.parameters()}\n",
    "            ], lr=output_lr, momentum=momentum\n",
    "        )\n",
    "    \n",
    "    # init the tensorboard\n",
    "    tensorboard_name = \"Fine_Tuning_With_Pretrain\" if pretrain else \"Fine_Tuning_Random_Initialize\"\n",
    "    writer = SummaryWriter(tensorboard_name, comment=\"-{}-{}-{}\".format(num_epoch, fine_tuning_lr, output_lr))\n",
    "        \n",
    "    # best accuracy\n",
    "    best_acc = 0.0\n",
    "    store_best_acc, count = [0 for _ in range(len(num_epochs))], 0\n",
    "    max_num_epoch = max(num_epochs)\n",
    "    \n",
    "    # iterate\n",
    "    for epoch in range(max_num_epoch):\n",
    "        # train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        samples = 0\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            samples += inputs.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / samples\n",
    "        print(\"[Epoch {:>2} / {:>2}], Training loss is {:>8.6f}\".format(epoch + 1, max_num_epoch, epoch_loss))\n",
    "        writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "\n",
    "        # test\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                running_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        writer.add_scalar('Validation/Loss', epoch_loss, epoch)\n",
    "        accuracy = correct / total\n",
    "        writer.add_scalar('Validation/Accuracy', accuracy, epoch)\n",
    "        print(\"[Epoch {:>2} / {:>2}], Validation loss is {:>8.6f}, Validation accuracy is {:>8.6f}\".format(\n",
    "            epoch + 1, max_num_epoch, epoch_loss, accuracy\n",
    "        ))\n",
    "        best_acc = max(best_acc, accuracy)\n",
    "        \n",
    "        if epoch + 1 == num_epochs[count]:\n",
    "            store_best_acc[count] = best_acc\n",
    "            count += 1\n",
    "\n",
    "    # close the tensorboard\n",
    "    writer.close()\n",
    "    \n",
    "    return store_best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'drive\\\\My Drive\\\\CUB-200-2011'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m GOOGLE_DRIVE_PATH_AFTER_MYDRIVE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUB-200-2011\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m GOOGLE_DRIVE_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Drive\u001b[39m\u001b[38;5;124m\"\u001b[39m, GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOOGLE_DRIVE_PATH\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add to sys so we can import .py files.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(GOOGLE_DRIVE_PATH) \n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'drive\\\\My Drive\\\\CUB-200-2011'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"CUB-200-2011\"\n",
    "\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "\n",
    "sys.path.append(GOOGLE_DRIVE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters here\n",
    "num_epochs = [10, 15, 20]\n",
    "fine_tuning_lrs = [0.0001, 0.0005, 0.01]\n",
    "output_lrs = [0.01, 0.02, 0.04, 0.06]\n",
    "\n",
    "configurations = list(product(fine_tuning_lrs, output_lrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accs = []\n",
    "\n",
    "# train with the pretrained model\n",
    "for config in configurations:\n",
    "    curr_best_acc = train_resnet_with_cub(num_epochs, fine_tuning_lr=config[0], output_lr=config[1])\n",
    "    best_accs.extend(curr_best_acc)\n",
    "    \n",
    "# write the results into a txt file\n",
    "with open('best_accuracy.txt', 'w') as f:\n",
    "    for config, accuracy in zip(list(product(fine_tuning_lrs, output_lrs, num_epochs)), best_accs):\n",
    "        f.write(f\"Configuration: {config}, Accuracy: {accuracy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
